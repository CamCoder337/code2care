from django.shortcuts import render, get_object_or_404
from django.http import JsonResponse, Http404, HttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.core.cache import cache
from django.core.paginator import Paginator
from rest_framework.permissions import AllowAny
from django.db.models import Count, Sum, Q, Avg
from django.db import transaction
from django.utils import timezone
from django.conf import settings
from datetime import datetime, timedelta, date
from rest_framework import generics, status
from rest_framework.decorators import api_view
from rest_framework.renderers import JSONRenderer, BrowsableAPIRenderer
from rest_framework.response import Response
from rest_framework.views import APIView
from rest_framework.pagination import PageNumberPagination
import json
import csv
import io
import logging
import time
import time
import sys
from django.core.cache import cache
from django.db import connection
from django.db.models import Count, Sum, Q, Avg
from django.db.models.functions import Extract
from django.db.models.functions import TruncMonth, TruncWeek, TruncDay
from django.utils import timezone
from datetime import datetime, timedelta, date
from .decorators import global_allow_any
from app.utils.cache_utils import safe_cache_get, safe_cache_set, cache_key_builder, cached_view
# Imports conditionnels pour les biblioth√®ques ML
try:
    import pandas as pd
    import numpy as np

    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False


    # Cr√©er des alternatives pour numpy si pas disponible
    class FakeNumpy:
        @staticmethod
        def random():
            import random
            class RandomModule:
                @staticmethod
                def poisson(lam, size=None):
                    if size is None:
                        return random.randint(max(1, int(lam * 0.7)), int(lam * 1.3))
                    return [random.randint(max(1, int(lam * 0.7)), int(lam * 1.3)) for _ in range(size)]

                @staticmethod
                def uniform(low, high, size=None):
                    if size is None:
                        return random.uniform(low, high)
                    return [random.uniform(low, high) for _ in range(size)]

            return RandomModule()

        @staticmethod
        def array(data):
            return data

        @staticmethod
        def std(data):
            if not data:
                return 0
            mean_val = sum(data) / len(data)
            return (sum((x - mean_val) ** 2 for x in data) / len(data)) ** 0.5

        @staticmethod
        def mean(data):
            return sum(data) / len(data) if data else 0

        @staticmethod
        def maximum(a, b):
            if isinstance(a, list) and isinstance(b, (int, float)):
                return [max(x, b) for x in a]
            return max(a, b)


    np = FakeNumpy()

try:
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.preprocessing import StandardScaler
    import joblib
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("Warning: scikit-learn not available")

from .models import (
    Donor, Site, Department, Patient, BloodRecord,
    BloodUnit, BloodRequest, BloodConsumption, Prevision
)
from .serializers import (
    DonorSerializer, SiteSerializer, DepartmentSerializer,
    PatientSerializer, BloodRecordSerializer, BloodUnitSerializer,
    BloodRequestSerializer, BloodConsumptionSerializer, PrevisionSerializer
)

try:
    from .forecasting.blood_demand_forecasting import RenderOptimizedForecaster, ProductionLightweightForecaster, \
    RealDataBloodDemandForecaster, XGBOOST_AVAILABLE, STATSMODELS_AVAILABLE

    ENHANCED_FORECASTING_AVAILABLE = True
except ImportError:
    ENHANCED_FORECASTING_AVAILABLE = False

logger = logging.getLogger(__name__)



#************************************************#


@global_allow_any
class StandardResultsSetPagination(PageNumberPagination):
    page_size = 10
    page_size_query_param = 'page_size'
    max_page_size = 100

#************************************************#
@global_allow_any
# Base class for all your API views
class BaseAPIView(APIView):
    """
    Base API view with proper renderer configuration
    """
    renderer_classes = [JSONRenderer, BrowsableAPIRenderer]
    permission_classes = []  # Override with your permission classes

    def handle_exception(self, exc):
        """Handle exceptions gracefully"""
        try:
            return super().handle_exception(exc)
        except Exception as e:
            return Response(
                {"error": f"An error occurred: {str(e)}"},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

@global_allow_any
# ==================== PAGINATION ====================
class StandardResultsSetPagination(PageNumberPagination):
    page_size = 20
    page_size_query_param = 'page_size'
    max_page_size = 100

@global_allow_any
# ==================== DASHBOARD VIEWS ====================
class DashboardOverviewAPIView(BaseAPIView):
    """Vue principale du dashboard avec m√©triques temps r√©el - VERSION OPTIMIS√âE avec Redis"""

    def get(self, request):
        # G√©n√©rer la cl√© de cache
        cache_key = cache_key_builder('dashboard_overview')

        # Tenter de r√©cup√©rer depuis le cache
        cached_result = safe_cache_get(cache_key)
        if cached_result:
            logger.info("Dashboard overview served from Redis cache")
            return Response(cached_result)

        try:
            # Votre logique existante pour g√©n√©rer les donn√©es
            start_time = time.time()

            # ==================== OPTIMISATION 1: Requ√™tes group√©es ====================
            # Une seule requ√™te pour toutes les statistiques d'unit√©s
            unit_stats = BloodUnit.objects.aggregate(
                total=Count('unit_id'),
                available=Count('unit_id', filter=Q(status='Available')),
                expired=Count('unit_id', filter=Q(status='Expired')),
                used=Count('unit_id', filter=Q(status='Used')),
                expiring_soon=Count(
                    'unit_id',
                    filter=Q(
                        status='Available',
                        date_expiration__lte=timezone.now().date() + timedelta(days=7)
                    )
                )
            )

            # ==================== OPTIMISATION 2: Stock par groupe sanguin optimis√© ====================
            stock_by_blood_type = list(
                BloodUnit.objects.filter(status='Available')
                .select_related('donor')  # √âviter les requ√™tes N+1
                .values('donor__blood_type')
                .annotate(
                    count=Count('unit_id'),
                    total_volume=Sum('volume_ml')
                )
                .order_by('donor__blood_type')
            )

            # ==================== OPTIMISATION 3: Requ√™tes de demandes group√©es ====================
            request_stats = BloodRequest.objects.aggregate(
                pending=Count('request_id', filter=Q(status='Pending')),
                urgent=Count('request_id', filter=Q(status='Pending', priority='Urgent'))
            )

            # ==================== OPTIMISATION 4: Transfusions aujourd'hui ====================
            today_transfusions = BloodConsumption.objects.filter(
                date=timezone.now().date()
            ).count()

            # ==================== OPTIMISATION 5: √âvolution simplifi√©e (√©chantillonnage) ====================
            # Au lieu de 30 jours, faire seulement 10 points pour r√©duire la charge
            stock_evolution = []
            today = timezone.now().date()

            # √âchantillonnage intelligent : 10 points sur 30 jours
            for i in range(0, 30, 3):  # Tous les 3 jours
                check_date = today - timedelta(days=29 - i)
                # Requ√™te optimis√©e avec un seul filtre
                daily_stock = BloodUnit.objects.filter(
                    collection_date__lte=check_date,
                    date_expiration__gt=check_date,
                    status__in=['Available', 'Used']  # Exclure les expir√©s
                ).count()

                stock_evolution.append({
                    'date': check_date.isoformat(),
                    'stock': daily_stock
                })

            # ==================== CALCUL DU TAUX D'UTILISATION ====================
            utilization_rate = 0
            if unit_stats['total'] > 0:
                utilization_rate = round((unit_stats['used'] / unit_stats['total'] * 100), 2)

            # ==================== STRUCTURE DE R√âPONSE ====================
            data = {
                'overview': {
                    'total_units': unit_stats['total'],
                    'available_units': unit_stats['available'],
                    'expired_units': unit_stats['expired'],
                    'used_units': unit_stats['used'],
                    'utilization_rate': utilization_rate,
                    'expiring_soon': unit_stats['expiring_soon'],
                    'pending_requests': request_stats['pending'],
                    'urgent_requests': request_stats['urgent'],
                    'today_transfusions': today_transfusions
                },
                'stock_by_blood_type': stock_by_blood_type,
                'stock_evolution': stock_evolution,
                'last_updated': timezone.now().isoformat(),
                'cache_info': {
                    'cached_at': timezone.now().isoformat(),
                    'cache_duration': '10 minutes',
                    'cache_backend': 'Redis Cloud'
                }
            }

            execution_time = time.time() - start_time
            logger.info(f"Dashboard overview generated in {execution_time:.2f}s")

            # ==================== CACHE REDIS ====================
            # Cache avec Redis Cloud pour 10 minutes
            safe_cache_set(cache_key, data, timeout=600)

            return Response(data)

        except Exception as e:
            logger.error(f"Dashboard error: {str(e)}", exc_info=True)

            # ==================== FALLBACK DATA ====================
            # Retourner des donn√©es minimales en cas d'erreur
            fallback_data = {
                'overview': {
                    'total_units': 0,
                    'available_units': 0,
                    'expired_units': 0,
                    'used_units': 0,
                    'utilization_rate': 0,
                    'expiring_soon': 0,
                    'pending_requests': 0,
                    'urgent_requests': 0,
                    'today_transfusions': 0
                },
                'stock_by_blood_type': [],
                'stock_evolution': [],
                'error': 'Donn√©es temporairement indisponibles',
                'last_updated': timezone.now().isoformat()
            }

            return Response(fallback_data, status=status.HTTP_206_PARTIAL_CONTENT)

@global_allow_any
class AlertsAPIView(BaseAPIView):
    """Alertes critiques pour le dashboard avec cache court"""

    def get(self, request):
        # Cache court pour les alertes (5 minutes max)
        cache_key = cache_key_builder('dashboard_alerts')
        cached_result = safe_cache_get(cache_key)

        if cached_result:
            logger.info("Alerts served from Redis cache")
            return Response(cached_result)

        alerts = []

        try:
            start_time = time.time()

            # Alertes stock faible
            for blood_type in ['A+', 'A-', 'B+', 'B-', 'AB+', 'AB-', 'O+', 'O-']:
                stock_count = BloodUnit.objects.filter(
                    donor__blood_type=blood_type,
                    status='Available'
                ).count()

                if stock_count < 5:  # Seuil critique
                    alerts.append({
                        'id': f'low_stock_{blood_type}',
                        'type': 'low_stock',
                        'severity': 'critical' if stock_count < 2 else 'warning',
                        'message': f'Stock critique pour {blood_type}: {stock_count} unit√©s',
                        'blood_type': blood_type,
                        'count': stock_count
                    })

            # Alertes expiration
            expiring_units = BloodUnit.objects.filter(
                status='Available',
                date_expiration__lte=timezone.now().date() + timedelta(days=3)
            ).select_related('donor')

            for unit in expiring_units:
                days_left = (unit.date_expiration - timezone.now().date()).days
                alerts.append({
                    'id': f'expiring_{unit.unit_id}',
                    'type': 'expiring',
                    'severity': 'critical' if days_left <= 1 else 'warning',
                    'message': f'Unit√© {unit.unit_id} expire dans {days_left} jour(s)',
                    'unit_id': unit.unit_id,
                    'blood_type': unit.donor.blood_type,
                    'days_left': days_left
                })

            # Alertes demandes urgentes non satisfaites
            urgent_requests = BloodRequest.objects.filter(
                status='Pending',
                priority='Urgent'
            ).select_related('department')

            for req in urgent_requests:
                alerts.append({
                    'id': f'urgent_{req.request_id}',
                    'type': 'urgent_request',
                    'severity': 'critical',
                    'message': f'Demande urgente {req.request_id} non satisfaite',
                    'request_id': req.request_id,
                    'blood_type': req.blood_type,
                    'department': req.department.name,
                    'quantity': req.quantity
                })

            execution_time = time.time() - start_time

            result = {
                'alerts': alerts,
                'count': len(alerts),
                'last_updated': timezone.now().isoformat(),
                'execution_time_seconds': round(execution_time, 2),
                'cache_backend': 'Redis Cloud'
            }

            # Cache court pour les alertes (5 minutes)
            safe_cache_set(cache_key, result, timeout=300)
            logger.info(f"Alerts generated and cached in {execution_time:.2f}s")

            return Response(result)

        except Exception as e:
            logger.error(f"Alerts error: {str(e)}")
            alerts.append({
                'id': 'system_error',
                'type': 'system_error',
                'severity': 'critical',
                'message': 'Erreur syst√®me lors du chargement des alertes'
            })

            return Response({
                'alerts': alerts,
                'count': len(alerts),
                'last_updated': timezone.now().isoformat()
            })

    def post(self, request):
        """Marquer toutes les alertes comme acquitt√©es"""
        try:
            action = request.data.get('action')
            if action == 'acknowledge_all':
                # Logique pour marquer les alertes comme vues
                return Response({
                    'success': True,
                    'message': 'Toutes les alertes ont √©t√© marqu√©es comme vues'
                })
            return Response({'error': 'Action non reconnue'}, status=400)
        except Exception as e:
            return Response({'error': str(e)}, status=500)

    def patch(self, request, alert_id=None):
        """R√©soudre une alerte sp√©cifique"""
        try:
            # R√©cup√©rer l'alert_id depuis l'URL
            alert_id = request.resolver_match.kwargs.get('alert_id')
            action = request.data.get('action')

            if action == 'resolve':
                # Logique pour r√©soudre l'alerte
                return Response({
                    'success': True,
                    'message': f'Alerte {alert_id} r√©solue'
                })
            return Response({'error': 'Action non reconnue'}, status=400)
        except Exception as e:
            return Response({'error': str(e)}, status=500)


@global_allow_any
# ==================== FORECASTING AI VIEWS ====================
class DemandForecastAPIView(APIView):
    """Vue optimis√©e pour les pr√©visions de demande avec Redis"""

    def __init__(self):
        super().__init__()
        self.lightweight_forecaster = ProductionLightweightForecaster()

    def post(self, request):
        """
        üî• NOUVELLE M√âTHODE POST pour correspondre au frontend React
        """
        try:
            # R√©cup√©rer les donn√©es du body de la requ√™te
            data = request.data

            blood_type = data.get('blood_type', 'all')
            days = int(data.get('forecast_period_days', 7))
            method = data.get('method')  # null pour auto-s√©lection
            include_confidence = data.get('include_confidence_intervals', True)
            include_optimization = data.get('include_optimization', True)

            # Limiter la dur√©e
            days = min(days, 30)

            # Cache par param√®tres
            cache_key = cache_key_builder('demand_forecast_post', blood_type, days, method)
            cached_result = safe_cache_get(cache_key)

            if cached_result:
                logger.info(f"POST Forecast for {blood_type} served from Redis cache")
                return Response(cached_result)

            start_time = time.time()

            # G√©n√©rer les pr√©dictions
            if blood_type == 'all':
                predictions = self.get_all_forecasts_optimized(days)
                main_forecast = predictions[0] if predictions else {}
            else:
                main_forecast = self.get_single_forecast_optimized(blood_type, days)
                predictions = [main_forecast]

            execution_time = time.time() - start_time

            # Format de r√©ponse compatible avec le frontend React
            result = {
                'predictions': main_forecast.get('predictions', []),
                'method_used': main_forecast.get('method_used', 'auto'),
                'blood_type': blood_type,
                'forecast_period_days': days,
                'generated_at': timezone.now().isoformat(),
                'generation_time_ms': int(execution_time * 1000),

                # M√©triques de pr√©cision (simul√©es si pas disponibles)
                'model_accuracy': {
                    'accuracy': '85%',
                    'samples': 1500,
                    'last_training': timezone.now().isoformat()
                },

                # Intervalles de confiance
                'confidence_intervals': {
                    'lower': [p.get('lower_bound', p.get('predicted_demand', 0) * 0.8)
                              for p in main_forecast.get('predictions', [])],
                    'upper': [p.get('upper_bound', p.get('predicted_demand', 0) * 1.2)
                              for p in main_forecast.get('predictions', [])]
                } if include_confidence else None,

                # Recommandations d'optimisation
                'optimization_recommendations': [
                    {
                        'type': 'stock_optimization',
                        'priority': 'high',
                        'message': f'Surveiller les niveaux de stock pour {blood_type}'
                    },
                    {
                        'type': 'collection_planning',
                        'priority': 'medium',
                        'message': 'Planifier les collectes selon la demande pr√©dite'
                    }
                ] if include_optimization else [],

                # Capacit√©s avanc√©es
                'enhanced_forecasting_available': True,

                # M√©tadonn√©es
                'metadata': {
                    'cache_duration': '1 hour',
                    'cache_backend': 'Redis Cloud',
                    'execution_time_seconds': round(execution_time, 2)
                }
            }

            # Cache long
            safe_cache_set(cache_key, result, timeout=3600)
            logger.info(f"POST Forecast generated and cached in {execution_time:.2f}s")

            return Response(result, status=status.HTTP_200_OK)

        except Exception as e:
            logger.error(f"POST Demand forecast failed: {e}")
            return Response({
                'error': f'Erreur lors de la g√©n√©ration: {str(e)}',
                'predictions': [],
                'generated_at': timezone.now().isoformat(),
                'method_used': 'error_fallback'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def get(self, request):
        """Pr√©visions optimis√©es avec cache Redis agressif"""

        # Param√®tres de la requ√™te
        blood_type = request.query_params.get('blood_type', 'all')
        days = int(request.query_params.get('days', 7))

        # Limiter la dur√©e pour √©viter les timeouts
        days = min(days, 30)

        # Cache par param√®tres avec Redis
        cache_key = cache_key_builder('demand_forecast', blood_type, days)
        cached_result = safe_cache_get(cache_key)

        if cached_result:
            logger.info(f"Forecast for {blood_type} served from Redis cache")
            return Response(cached_result)

        try:
            start_time = time.time()

            if blood_type == 'all':
                forecasts = self.get_all_forecasts_optimized(days)
            else:
                forecasts = [self.get_single_forecast_optimized(blood_type, days)]

            execution_time = time.time() - start_time

            result = {
                'forecasts': forecasts,
                'parameters': {
                    'blood_type': blood_type,
                    'days_ahead': days,
                    'generated_at': timezone.now().isoformat()
                },
                'metadata': {
                    'method': 'lightweight_optimized',
                    'confidence_level': 0.75,
                    'cache_duration': '1 hour',
                    'cache_backend': 'Redis Cloud',
                    'execution_time_seconds': round(execution_time, 2)
                }
            }

            # Cache long pour les pr√©visions avec Redis
            safe_cache_set(cache_key, result, timeout=3600)  # 1 heure
            logger.info(f"Forecast generated and cached in {execution_time:.2f}s")

            return Response(result)

        except Exception as e:
            logger.error(f"Demand forecast failed: {e}")
            return Response({
                'error': 'Service temporairement indisponible',
                'forecasts': [],
                'generated_at': timezone.now().isoformat()
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)

    def get_all_forecasts_optimized(self, days):
        """Pr√©visions pour tous les groupes sanguins - Version rapide avec cache individuel"""
        blood_types = ['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-']
        forecasts = []

        for bt in blood_types:
            try:
                # Cache individuel par groupe sanguin
                individual_cache_key = cache_key_builder('forecast_individual', bt, days)
                cached_forecast = safe_cache_get(individual_cache_key)

                if cached_forecast:
                    forecasts.append(cached_forecast)
                else:
                    forecast = self.lightweight_forecaster.quick_predict_cached(bt, days)
                    safe_cache_set(individual_cache_key, forecast, timeout=1800)  # 30 minutes
                    forecasts.append(forecast)

            except Exception as e:
                logger.warning(f"Forecast failed for {bt}: {e}")
                # Ajouter pr√©vision minimale
                forecasts.append({
                    'blood_type': bt,
                    'predictions': [],
                    'method_used': 'error_fallback',
                    'error': str(e)
                })

        return forecasts

    def get_single_forecast_optimized(self, blood_type, days):
        """Pr√©vision pour un seul groupe sanguin avec cache"""
        try:
            # Cache sp√©cifique au groupe sanguin
            individual_cache_key = cache_key_builder('forecast_single', blood_type, days)
            cached_forecast = safe_cache_get(individual_cache_key)

            if cached_forecast:
                return cached_forecast

            forecast = self.lightweight_forecaster.quick_predict_cached(blood_type, days)
            safe_cache_set(individual_cache_key, forecast, timeout=1800)  # 30 minutes
            return forecast

        except Exception as e:
            logger.error(f"Single forecast failed for {blood_type}: {e}")
            return {
                'blood_type': blood_type,
                'predictions': [],
                'method_used': 'error_fallback',
                'error': str(e)
            }

@global_allow_any
class SmartForecastView(APIView):
    """
    üß† Enhanced Smart Forecast View - AI SYSTEM ONLY
    Version optimis√©e pour utiliser uniquement le syst√®me IA
    """
    permission_classes = [AllowAny]

    def get(self, request):
        """GET method - always uses AI system"""
        return self.handle_ai_forecast_request(request, is_post=False)

    def post(self, request):
        """POST method - always uses AI system"""
        return self.handle_ai_forecast_request(request, is_post=True)

    def handle_ai_forecast_request(self, request, is_post=True):
        """Unified AI-only request handler"""
        try:
            # Extract parameters
            if is_post:
                data = request.data
                blood_type = data.get('blood_type', 'O+')
                days = int(data.get('days_ahead', 7))
                method = data.get('method', 'auto')
                force_retrain = data.get('force_retrain', False)
            else:
                blood_type = request.query_params.get('blood_type', 'O+')
                days = int(request.query_params.get('days', 7))
                method = request.query_params.get('method', 'auto')
                force_retrain = request.query_params.get('force_retrain', 'false').lower() == 'true'

            # Validate parameters
            if days > 30:
                days = 30
            if days < 1:
                days = 7

            # Validate method
            valid_methods = ['auto', 'random_forest', 'xgboost', 'arima', 'stl_arima']
            if method not in valid_methods:
                method = 'auto'

            # Log the request
            logger.info(f"ü§ñ AI Forecast Request: {blood_type}, {days} days, method: {method}")

            # Always use AI system - no fallback to classic
            return self.handle_ai_forecast_only(blood_type, days, method, force_retrain)

        except ValueError as e:
            logger.error(f"Invalid parameters: {e}")
            return Response({
                'error': 'Invalid parameters',
                'message': str(e),
                'timestamp': timezone.now().isoformat(),
                'suggestions': {
                    'blood_type': 'Use standard blood types (O+, A+, B+, AB+, O-, A-, B-, AB-)',
                    'days': 'Use values between 1 and 30',
                    'method': 'Use auto, random_forest, xgboost, arima, or stl_arima'
                }
            }, status=status.HTTP_400_BAD_REQUEST)

        except Exception as e:
            logger.error(f"Forecast request failed: {e}")
            return Response({
                'error': 'Forecast generation failed',
                'message': str(e),
                'timestamp': timezone.now().isoformat(),
                'system': 'ai_only'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def handle_ai_forecast_only(self, blood_type, days, method, force_retrain):
        """Handle AI-powered forecasting with robust error handling"""
        try:
            # Import AI forecasting function with error handling
            try:
                from forecasting.blood_demand_forecasting import generate_forecast_api
                logger.info(f"‚úÖ AI module imported successfully")
            except ImportError as import_error:
                logger.error(f"‚ùå AI forecasting module not available: {import_error}")
                return self.handle_ai_import_error(blood_type, days, method)

            logger.info(f"ü§ñ Using AI system for {blood_type} forecast (method: {method})")

            # Generate AI forecast with timeout protection
            try:
                result = generate_forecast_api(
                    blood_type=blood_type,
                    days_ahead=days,
                    method=method,
                    force_retrain=force_retrain
                )

                logger.info(f"‚úÖ AI forecast generated successfully for {blood_type}")

            except Exception as forecast_error:
                logger.error(f"‚ùå AI forecast generation failed: {forecast_error}")
                return self.handle_ai_forecast_error(blood_type, days, method, forecast_error)

            # Check if result contains error
            if result.get('error'):
                logger.warning(f"‚ö†Ô∏è AI system returned error: {result.get('error')}")
                return self.handle_ai_result_error(result, blood_type, days, method)

            # Adapt result for frontend compatibility
            try:
                adapted_result = self.adapt_ai_result_for_frontend(result, days)

                # Add system metadata
                adapted_result.update({
                    'system_used': 'ai_only',
                    'classic_fallback': False,
                    'timestamp': timezone.now().isoformat(),
                    'request_method': method,
                    'ai_system_status': 'operational'
                })

                logger.info(f"‚úÖ AI forecast successfully adapted for {blood_type}")
                return Response(adapted_result, status=status.HTTP_200_OK)

            except Exception as adapt_error:
                logger.error(f"‚ùå Error adapting AI result: {adapt_error}")
                return self.handle_adaptation_error(result, blood_type, days, adapt_error)

        except Exception as e:
            logger.error(f"‚ùå Critical AI forecast error: {e}")
            return self.handle_critical_ai_error(blood_type, days, method, e)

    def handle_ai_import_error(self, blood_type, days, method):
        """Handle AI module import errors"""
        return Response({
            'error': 'AI system unavailable',
            'message': 'AI forecasting module could not be imported',
            'blood_type': blood_type,
            'forecast_period_days': days,
            'requested_method': method,
            'system_status': 'ai_module_missing',
            'timestamp': timezone.now().isoformat(),
            'recommendation': 'Please ensure AI forecasting dependencies are installed',
            'fallback_available': False
        }, status=status.HTTP_503_SERVICE_UNAVAILABLE)

    def handle_ai_forecast_error(self, blood_type, days, method, error):
        """Handle AI forecast generation errors"""
        return Response({
            'error': 'AI forecast generation failed',
            'message': str(error),
            'blood_type': blood_type,
            'forecast_period_days': days,
            'requested_method': method,
            'system_status': 'ai_generation_error',
            'timestamp': timezone.now().isoformat(),
            'error_type': type(error).__name__,
            'fallback_available': False
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def handle_ai_result_error(self, result, blood_type, days, method):
        """Handle errors returned by AI system"""
        error_msg = result.get('message', 'Unknown AI system error')

        return Response({
            'error': 'AI system error',
            'message': error_msg,
            'blood_type': blood_type,
            'forecast_period_days': days,
            'requested_method': method,
            'ai_error_details': result.get('error'),
            'system_status': 'ai_internal_error',
            'timestamp': timezone.now().isoformat(),
            'fallback_available': False
        }, status=status.HTTP_400_BAD_REQUEST)

    def handle_adaptation_error(self, original_result, blood_type, days, error):
        """Handle result adaptation errors - return original result"""
        logger.warning(f"‚ö†Ô∏è Adaptation failed, returning original AI result")

        # Try to return original result with minimal modification
        try:
            original_result.update({
                'system_used': 'ai_only',
                'adaptation_warning': f'Result adaptation failed: {str(error)}',
                'timestamp': timezone.now().isoformat()
            })
            return Response(original_result, status=status.HTTP_200_OK)
        except:
            return Response({
                'error': 'Result adaptation failed',
                'message': str(error),
                'blood_type': blood_type,
                'forecast_period_days': days,
                'timestamp': timezone.now().isoformat(),
                'fallback_available': False
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def handle_critical_ai_error(self, blood_type, days, method, error):
        """Handle critical AI system errors"""
        return Response({
            'error': 'Critical AI system failure',
            'message': str(error),
            'blood_type': blood_type,
            'forecast_period_days': days,
            'requested_method': method,
            'system_status': 'critical_failure',
            'timestamp': timezone.now().isoformat(),
            'error_type': type(error).__name__,
            'recommendation': 'Please check AI system health and try again',
            'fallback_available': False
        }, status=status.HTTP_503_SERVICE_UNAVAILABLE)

    def adapt_ai_result_for_frontend(self, ai_result, days):
        """Adapt AI result for frontend compatibility with robust error handling"""
        try:
            # Ensure essential fields exist
            adapted = {
                'predictions': ai_result.get('predictions', []),
                'method_used': ai_result.get('method_used', 'unknown'),
                'blood_type': ai_result.get('blood_type', 'unknown'),
                'forecast_period_days': days,
                'generated_at': ai_result.get('generated_at', timezone.now().isoformat()),
                'data_source': ai_result.get('data_source', 'ai_system'),
                'enhanced_forecasting_available': True,
                'system_type': 'ai_only'
            }

            # Add optional fields if available
            optional_fields = [
                'model_performance',
                'confidence_intervals',
                'contextual_insights',
                'optimization_recommendations',
                'quality_metrics',
                'warning'
            ]

            for field in optional_fields:
                if field in ai_result:
                    adapted[field] = ai_result[field]

            # Validate predictions structure
            if adapted['predictions']:
                validated_predictions = []
                for pred in adapted['predictions']:
                    if isinstance(pred, dict) and 'date' in pred and 'predicted_demand' in pred:
                        validated_predictions.append({
                            'date': pred['date'],
                            'predicted_demand': max(0, int(pred.get('predicted_demand', 0))),
                            'confidence': float(pred.get('confidence', 0.5))
                        })
                adapted['predictions'] = validated_predictions

            # Add metadata
            adapted.update({
                'adaptation_timestamp': timezone.now().isoformat(),
                'result_validated': True,
                'prediction_count': len(adapted['predictions'])
            })

            return adapted

        except Exception as e:
            logger.error(f"‚ùå Critical adaptation error: {e}")
            # Return minimal valid structure
            return {
                'error': 'Adaptation failed',
                'message': str(e),
                'blood_type': ai_result.get('blood_type', 'unknown'),
                'forecast_period_days': days,
                'predictions': [],
                'method_used': 'error',
                'generated_at': timezone.now().isoformat(),
                'system_type': 'ai_only',
                'adaptation_failed': True
            }


@global_allow_any
class AISystemHealthView(APIView):
    """
    üè• Endpoint de sant√© et debug pour le syst√®me IA
    """
    permission_classes = [AllowAny]

    def get(self, request):
        """V√©rification compl√®te du syst√®me IA"""
        try:
            from forecasting.blood_demand_forecasting import health_check, get_available_methods

            # Test d'import
            try:
                from forecasting.blood_demand_forecasting import RealDataBloodDemandForecaster
                forecaster_available = True
            except ImportError as e:
                forecaster_available = False
                import_error = str(e)

            # V√©rification des d√©pendances
            dependencies = {
                'pandas': self.check_import('pandas'),
                'numpy': self.check_import('numpy'),
                'sklearn': self.check_import('sklearn'),
                'xgboost': self.check_import('xgboost'),
                'statsmodels': self.check_import('statsmodels'),
                'django': self.check_import('django')
            }

            # Test de connexion base de donn√©es
            db_status = self.check_database_connection()

            # Test du syst√®me IA
            ai_health = health_check()
            available_methods = get_available_methods()

            # Test rapide de pr√©diction
            quick_test = self.quick_forecast_test()

            return Response({
                'system_status': 'healthy' if forecaster_available and db_status['connected'] else 'degraded',
                'timestamp': timezone.now().isoformat(),
                'components': {
                    'forecaster_module': forecaster_available,
                    'database': db_status,
                    'dependencies': dependencies,
                    'ai_system': ai_health,
                    'available_methods': available_methods,
                    'quick_test': quick_test
                },
                'recommendations': self.get_system_recommendations(
                    forecaster_available, db_status, dependencies, quick_test
                )
            })

        except Exception as e:
            return Response({
                'system_status': 'error',
                'error': str(e),
                'timestamp': timezone.now().isoformat()
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def check_import(self, module_name):
        """Test d'import d'un module"""
        try:
            __import__(module_name)
            return {'available': True, 'version': 'unknown'}
        except ImportError as e:
            return {'available': False, 'error': str(e)}

    def check_database_connection(self):
        """Test de connexion √† la base de donn√©es"""
        try:
            from django.db import connection
            from .models import BloodInventory, Transaction

            with connection.cursor() as cursor:
                cursor.execute("SELECT 1")

            # Test des mod√®les
            inventory_count = BloodInventory.objects.count()
            transaction_count = Transaction.objects.count()

            return {
                'connected': True,
                'inventory_records': inventory_count,
                'transaction_records': transaction_count,
                'has_data': inventory_count > 0 or transaction_count > 0
            }

        except Exception as e:
            return {
                'connected': False,
                'error': str(e)
            }

    def quick_forecast_test(self):
        """Test rapide de g√©n√©ration de pr√©vision"""
        try:
            from forecasting.blood_demand_forecasting import generate_forecast_api

            # Test avec O+ (type courant)
            start_time = time.time()
            result = generate_forecast_api('O+', days_ahead=3, method='auto')
            processing_time = time.time() - start_time

            success = 'predictions' in result and len(result.get('predictions', [])) > 0

            return {
                'success': success,
                'processing_time_seconds': round(processing_time, 2),
                'method_used': result.get('method_used', 'unknown'),
                'prediction_count': len(result.get('predictions', [])),
                'has_error': 'error' in result,
                'error_message': result.get('error') if 'error' in result else None
            }

        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def get_system_recommendations(self, forecaster_available, db_status, dependencies, quick_test):
        """G√©n√©rer des recommandations bas√©es sur l'√©tat du syst√®me"""
        recommendations = []

        if not forecaster_available:
            recommendations.append({
                'type': 'critical',
                'message': 'Le module de pr√©vision IA n\'est pas disponible',
                'action': 'V√©rifier l\'installation et les imports'
            })

        if not db_status.get('connected'):
            recommendations.append({
                'type': 'critical',
                'message': 'Connexion base de donn√©es √©chou√©e',
                'action': 'V√©rifier la configuration de la base de donn√©es'
            })

        if not db_status.get('has_data'):
            recommendations.append({
                'type': 'warning',
                'message': 'Aucune donn√©e trouv√©e en base',
                'action': 'Charger des donn√©es d\'exemple ou v√©rifier les mod√®les'
            })

        if not dependencies.get('xgboost', {}).get('available'):
            recommendations.append({
                'type': 'info',
                'message': 'XGBoost non disponible',
                'action': 'pip install xgboost pour am√©liorer les performances'
            })

        if not dependencies.get('statsmodels', {}).get('available'):
            recommendations.append({
                'type': 'info',
                'message': 'Statsmodels non disponible',
                'action': 'pip install statsmodels pour les mod√®les ARIMA'
            })

        if not quick_test.get('success'):
            recommendations.append({
                'type': 'warning',
                'message': 'Test rapide de pr√©vision √©chou√©',
                'action': 'V√©rifier les logs pour plus de d√©tails'
            })

        if quick_test.get('processing_time_seconds', 0) > 10:
            recommendations.append({
                'type': 'performance',
                'message': 'Temps de traitement √©lev√©',
                'action': 'Optimiser les performances ou augmenter le cache'
            })

        if not recommendations:
            recommendations.append({
                'type': 'success',
                'message': 'Syst√®me IA fonctionnel',
                'action': 'Aucune action requise'
            })

        return recommendations

@global_allow_any
class AIMethodsView(APIView):
    """
    üîß Endpoint pour tester les diff√©rentes m√©thodes IA
    """
    permission_classes = [AllowAny]

    def get(self, request):
        """Liste des m√©thodes disponibles"""
        try:
            from forecasting.blood_demand_forecasting import get_available_methods

            methods_info = get_available_methods()

            return Response({
                'available_methods': methods_info['available_methods'],
                'system_capabilities': methods_info['system_capabilities'],
                'timestamp': timezone.now().isoformat()
            })

        except Exception as e:
            return Response({
                'error': 'Failed to get methods info',
                'message': str(e),
                'timestamp': timezone.now().isoformat()
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def post(self, request):
        """Test d'une m√©thode sp√©cifique"""
        try:
            blood_type = request.data.get('blood_type', 'O+')
            method = request.data.get('method', 'auto')
            days = int(request.data.get('days', 7))

            from forecasting.blood_demand_forecasting import generate_forecast_api

            start_time = time.time()
            result = generate_forecast_api(blood_type, days, method, force_retrain=True)
            processing_time = time.time() - start_time

            # Analyser le r√©sultat
            analysis = {
                'success': 'predictions' in result and len(result.get('predictions', [])) > 0,
                'processing_time_seconds': round(processing_time, 2),
                'method_actually_used': result.get('method_used'),
                'prediction_count': len(result.get('predictions', [])),
                'has_confidence_intervals': 'confidence_intervals' in result,
                'has_performance_metrics': 'model_performance' in result,
                'data_source': result.get('data_source'),
                'warning_present': 'warning' in result
            }

            return Response({
                'test_parameters': {
                    'blood_type': blood_type,
                    'method_requested': method,
                    'days': days
                },
                'result': result,
                'analysis': analysis,
                'timestamp': timezone.now().isoformat()
            })

        except Exception as e:
            return Response({
                'error': 'Method test failed',
                'message': str(e),
                'timestamp': timezone.now().isoformat()
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)



@global_allow_any
# ==================== OPTIMIZATION VIEWS ====================
class OptimizationRecommendationsAPIView(APIView):
    """Vue optimis√©e pour les recommandations avec Redis Cache"""

    def __init__(self):
        super().__init__()
        self.forecaster = RenderOptimizedForecaster(max_execution_time=120)  # 2 minutes max
        self.lightweight_forecaster = ProductionLightweightForecaster()

    def get(self, request):
        """Recommandations optimis√©es avec cache Redis intelligent"""
        start_time = time.time()

        # ==================== CACHE REDIS PRINCIPAL ====================
        cache_key = cache_key_builder('optimization_recommendations_v2')
        cached_data = safe_cache_get(cache_key)

        if cached_data:
            logger.info("Using cached recommendations from Redis")
            return Response(cached_data)

        try:
            # ==================== STRAT√âGIE PROGRESSIVE AVEC CACHE ====================
            recommendations = self.generate_progressive_recommendations_cached()

            execution_time = time.time() - start_time

            data = {
                'recommendations': recommendations,
                'generated_at': timezone.now().isoformat(),
                'execution_time_seconds': round(execution_time, 2),
                'cache_duration': '30 minutes',
                'cache_backend': 'Redis Cloud',
                'status': 'success'
            }

            # ==================== CACHE ADAPTATIF REDIS ====================
            # Cache plus long si g√©n√©ration rapide (donn√©es stables)
            # Cache plus court si g√©n√©ration lente (donn√©es dynamiques)
            cache_duration = 1800 if execution_time < 30 else 900  # 30min ou 15min
            safe_cache_set(cache_key, data, timeout=cache_duration)

            logger.info(f"Recommendations generated and cached in {execution_time:.2f}s")

            return Response(data)

        except Exception as e:
            logger.error(f"Recommendations generation failed: {str(e)}", exc_info=True)
            return self.get_emergency_fallback()

    def generate_progressive_recommendations_cached(self):
        """G√©n√©ration progressive avec cache par √©tapes"""

        try:
            recommendations = {
                'blood_type_specific': [],
                'general': [],
                'summary': {}
            }

            blood_types = ['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-']
            successful_forecasts = 0

            # ==================== CACHE PAR GROUPE SANGUIN ====================
            for blood_type in blood_types:
                try:
                    # Cache individuel pour chaque analyse
                    blood_type_cache_key = cache_key_builder('recommendation_blood_type', blood_type)
                    cached_recommendation = safe_cache_get(blood_type_cache_key)

                    if cached_recommendation:
                        recommendations['blood_type_specific'].append(cached_recommendation)
                        successful_forecasts += 1
                        continue

                    # Timeout individuel par groupe sanguin (15 secondes max)
                    recommendation = self.analyze_blood_type_optimized(blood_type, timeout=15)

                    if recommendation:
                        # Cache la recommandation individuelle pour 15 minutes
                        safe_cache_set(blood_type_cache_key, recommendation, timeout=900)
                        recommendations['blood_type_specific'].append(recommendation)
                        successful_forecasts += 1

                    # Si trop lent, basculer sur m√©thode ultra-rapide
                    if successful_forecasts < len(blood_types) // 2 and len(recommendations['blood_type_specific']) > 0:
                        # Compl√©ter avec m√©thode rapide
                        remaining_types = blood_types[len(recommendations['blood_type_specific']):]
                        for bt in remaining_types:
                            quick_rec = self.quick_analyze_blood_type(bt)
                            recommendations['blood_type_specific'].append(quick_rec)
                        break

                except Exception as e:
                    logger.warning(f"Failed to analyze {blood_type}: {e}")
                    # Ajouter recommandation de base
                    recommendations['blood_type_specific'].append(
                        self.get_basic_recommendation(blood_type)
                    )

            # ==================== CACHE RECOMMANDATIONS G√âN√âRALES ====================
            general_cache_key = cache_key_builder('recommendations_general')
            cached_general = safe_cache_get(general_cache_key)

            if cached_general:
                recommendations['general'] = cached_general
            else:
                general_recs = self.generate_general_recommendations_fast()
                safe_cache_set(general_cache_key, general_recs, timeout=600)  # 10 minutes
                recommendations['general'] = general_recs

            # ==================== R√âSUM√â ====================
            recommendations['summary'] = self.generate_summary_optimized(
                recommendations['blood_type_specific']
            )

            return recommendations

        except Exception as e:
            logger.error(f"Progressive recommendations failed: {e}")
            return self.get_static_recommendations()


    def analyze_blood_type_optimized(self, blood_type, timeout=15):
        """Analyse optimis√©e d'un groupe sanguin avec timeout"""
        start_time = time.time()

        try:
            # ==================== DONN√âES ACTUELLES RAPIDES ====================
            current_stock = BloodUnit.objects.filter(
                donor__blood_type=blood_type,
                status='Available'
            ).count()

            # ==================== CONSOMMATION R√âCENTE ====================
            seven_days_ago = timezone.now().date() - timedelta(days=7)
            recent_consumption = BloodConsumption.objects.filter(
                unit__donor__blood_type=blood_type,
                date__gte=seven_days_ago
            ).count()

            # ==================== PR√âDICTION RAPIDE ====================
            if time.time() - start_time > timeout * 0.7:  # 70% du timeout √©coul√©
                # Utiliser pr√©diction ultra-rapide
                prediction_result = self.lightweight_forecaster.quick_predict_cached(blood_type, 7)
                predicted_weekly_demand = sum(p['predicted_demand'] for p in prediction_result['predictions'])
                method_used = 'lightweight'
            else:
                # Essayer pr√©diction avanc√©e
                try:
                    historical_data = self.get_historical_data_fast(blood_type, days=30)
                    if len(historical_data) > 10:
                        # Entra√Ænement rapide
                        self.forecaster.train_comprehensive_optimized(historical_data, blood_type)
                        prediction_result = self.forecaster.predict_hybrid_optimized(blood_type, 7)
                        predicted_weekly_demand = sum(p['predicted_demand'] for p in prediction_result['predictions'])
                        method_used = 'optimized'
                    else:
                        raise ValueError("Insufficient historical data")
                except:
                    # Fallback vers m√©thode l√©g√®re
                    prediction_result = self.lightweight_forecaster.quick_predict_cached(blood_type, 7)
                    predicted_weekly_demand = sum(p['predicted_demand'] for p in prediction_result['predictions'])
                    method_used = 'lightweight_fallback'

            # ==================== UNIT√âS EXPIRANT ====================
            expiring_soon = BloodUnit.objects.filter(
                donor__blood_type=blood_type,
                status='Available',
                date_expiration__lte=timezone.now().date() + timedelta(days=7)
            ).count()

            # ==================== CALCULS DE RECOMMANDATION ====================
            daily_avg_consumption = recent_consumption / 7 if recent_consumption > 0 else 0.5

            return self.generate_blood_type_recommendation_optimized(
                blood_type, current_stock, predicted_weekly_demand,
                expiring_soon, daily_avg_consumption, method_used
            )

        except Exception as e:
            logger.warning(f"Optimized analysis failed for {blood_type}: {e}")
            return self.get_basic_recommendation(blood_type)

    def quick_analyze_blood_type(self, blood_type):
        """Analyse ultra-rapide pour fallback"""
        try:
            # Donn√©es minimales
            current_stock = BloodUnit.objects.filter(
                donor__blood_type=blood_type,
                status='Available'
            ).count()

            # Pr√©diction statique bas√©e sur des moyennes
            base_demands = {
                'O+': 15, 'A+': 12, 'B+': 8, 'AB+': 3,
                'O-': 7, 'A-': 6, 'B-': 4, 'AB-': 2
            }
            predicted_weekly_demand = base_demands.get(blood_type, 10) * 7

            expiring_soon = BloodUnit.objects.filter(
                donor__blood_type=blood_type,
                status='Available',
                date_expiration__lte=timezone.now().date() + timedelta(days=7)
            ).count()

            return self.generate_blood_type_recommendation_optimized(
                blood_type, current_stock, predicted_weekly_demand,
                expiring_soon, predicted_weekly_demand / 7, 'quick_static'
            )

        except Exception as e:
            logger.error(f"Quick analysis failed for {blood_type}: {e}")
            return self.get_basic_recommendation(blood_type)

    def get_historical_data_fast(self, blood_type, days=30):
        """R√©cup√©ration rapide des donn√©es historiques"""
        try:
            end_date = timezone.now().date()
            start_date = end_date - timedelta(days=days)

            # Agr√©gation par jour
            daily_consumption = BloodConsumption.objects.filter(
                unit__donor__blood_type=blood_type,
                date__gte=start_date,
                date__lte=end_date
            ).extra(
                select={'day': 'date'}
            ).values('day').annotate(
                demand=Count('id')
            ).order_by('day')

            if not daily_consumption:
                # Donn√©es simul√©es si pas d'historique
                dates = pd.date_range(start=start_date, end=end_date, freq='D')
                base_demand = {'O+': 2, 'A+': 1.5, 'B+': 1, 'AB+': 0.5,
                               'O-': 1, 'A-': 0.8, 'B-': 0.6, 'AB-': 0.3}.get(blood_type, 1)

                data = pd.DataFrame({
                    'demand': [max(0, int(base_demand + np.random.normal(0, 0.5))) for _ in dates]
                }, index=dates)
                return data

            # Convertir en DataFrame
            df = pd.DataFrame(list(daily_consumption))
            df['day'] = pd.to_datetime(df['day'])
            df.set_index('day', inplace=True)

            # Remplir les jours manquants avec 0
            full_range = pd.date_range(start=start_date, end=end_date, freq='D')
            df = df.reindex(full_range, fill_value=0)

            return df

        except Exception as e:
            logger.error(f"Historical data retrieval failed for {blood_type}: {e}")
            return pd.DataFrame()

    def generate_blood_type_recommendation_optimized(self, blood_type, current_stock,
                                                     predicted_demand, expiring_soon,
                                                     daily_avg, method_used):
        """G√©n√©ration optimis√©e de recommandation"""

        # ==================== LOGIQUE DE RECOMMANDATION ====================
        stock_ratio = current_stock / max(predicted_demand, 1)
        days_of_supply = current_stock / max(daily_avg, 0.1)

        # D√©termination du niveau de priorit√©
        if stock_ratio < 0.3 or days_of_supply < 3:
            priority = 'CRITICAL'
            action = 'EMERGENCY_COLLECTION'
            message = f"Stock critique pour {blood_type}. Collecte d'urgence requise."
        elif stock_ratio < 0.6 or days_of_supply < 7:
            priority = 'HIGH'
            action = 'URGENT_COLLECTION'
            message = f"Stock faible pour {blood_type}. Collection urgente n√©cessaire."
        elif stock_ratio < 1.0 or days_of_supply < 14:
            priority = 'MEDIUM'
            action = 'SCHEDULE_COLLECTION'
            message = f"Stock mod√©r√© pour {blood_type}. Programmer une collecte."
        elif expiring_soon > current_stock * 0.25:
            priority = 'MEDIUM'
            action = 'USE_EXPIRING_UNITS'
            message = f"Nombreuses unit√©s {blood_type} expirant bient√¥t. Prioriser leur utilisation."
        else:
            priority = 'LOW'
            action = 'MONITOR'
            message = f"Stock {blood_type} stable. Continuer la surveillance."

        return {
            'blood_type': blood_type,
            'current_stock': current_stock,
            'predicted_weekly_demand': round(predicted_demand, 1),
            'daily_average_consumption': round(daily_avg, 2),
            'expiring_soon': expiring_soon,
            'stock_ratio': round(stock_ratio, 2),
            'days_of_supply': round(days_of_supply, 1),
            'priority': priority,
            'recommended_action': action,
            'message': message,
            'prediction_method': method_used,
            'analysis_timestamp': timezone.now().isoformat()
        }

    def get_basic_recommendation(self, blood_type):
        """Recommandation de base en cas d'erreur"""
        return {
            'blood_type': blood_type,
            'current_stock': 0,
            'predicted_weekly_demand': 10.0,
            'daily_average_consumption': 1.5,
            'expiring_soon': 0,
            'stock_ratio': 0.0,
            'days_of_supply': 0.0,
            'priority': 'UNKNOWN',
            'recommended_action': 'CHECK_MANUALLY',
            'message': f"Donn√©es indisponibles pour {blood_type}. V√©rification manuelle requise.",
            'prediction_method': 'fallback',
            'analysis_timestamp': timezone.now().isoformat()
        }

    def generate_general_recommendations_fast(self):
        """Recommandations g√©n√©rales rapides"""
        try:
            recommendations = []

            # ==================== STATISTIQUES RAPIDES ====================
            unit_stats = BloodUnit.objects.aggregate(
                total=Count('unit_id'),
                expired=Count('unit_id', filter=Q(status='Expired')),
                expiring_week=Count('unit_id', filter=Q(
                    status='Available',
                    date_expiration__lte=timezone.now().date() + timedelta(days=7)
                ))
            )

            urgent_requests = BloodRequest.objects.filter(
                status='Pending',
                priority='Urgent'
            ).count()

            # ==================== RECOMMANDATIONS BAS√âES SUR LES STATS ====================
            if unit_stats['total'] > 0:
                expiry_rate = (unit_stats['expired'] / unit_stats['total']) * 100
                if expiry_rate > 15:
                    recommendations.append({
                        'type': 'WASTE_REDUCTION',
                        'priority': 'HIGH',
                        'message': f"Taux d'expiration √©lev√© ({expiry_rate:.1f}%). Optimiser la rotation FIFO.",
                        'metric': round(expiry_rate, 1),
                        'action': 'Impl√©menter un syst√®me de rotation strict'
                    })

            if unit_stats['expiring_week'] > 5:
                recommendations.append({
                    'type': 'EXPIRY_ALERT',
                    'priority': 'HIGH',
                    'message': f"{unit_stats['expiring_week']} unit√©s expirent cette semaine. Action imm√©diate requise.",
                    'metric': unit_stats['expiring_week'],
                    'action': 'Prioriser l\'utilisation des unit√©s √† expiration proche'
                })

            if urgent_requests > 0:
                recommendations.append({
                    'type': 'URGENT_PROCESSING',
                    'priority': 'CRITICAL',
                    'message': f"{urgent_requests} demande(s) urgente(s) en attente. Traitement imm√©diat requis.",
                    'metric': urgent_requests,
                    'action': 'Traiter les demandes urgentes en priorit√©'
                })

            # ==================== RECOMMANDATION SAISONNI√àRE ====================
            current_month = timezone.now().month
            seasonal_rec = self.get_seasonal_recommendation(current_month)
            if seasonal_rec:
                recommendations.append(seasonal_rec)

            return recommendations

        except Exception as e:
            logger.error(f"General recommendations failed: {e}")
            return [{
                'type': 'SYSTEM_ERROR',
                'priority': 'LOW',
                'message': 'Donn√©es de recommandations g√©n√©rales temporairement indisponibles.',
                'metric': 0,
                'action': 'Utiliser les proc√©dures manuelles standard'
            }]

    def get_seasonal_recommendation(self, month):
        """Recommandation saisonni√®re"""
        seasonal_patterns = {
            (6, 7, 8): {  # √ât√©
                'type': 'SEASONAL_SUMMER',
                'priority': 'MEDIUM',
                'message': 'P√©riode estivale: Intensifier les campagnes de collecte (baisse des dons pendant les vacances).',
                'action': 'Planifier des collectes mobiles et √©v√©nements sp√©ciaux'
            },
            (11, 12, 1): {  # Fin d'ann√©e / Nouvel an
                'type': 'SEASONAL_YEAR_END',
                'priority': 'MEDIUM',
                'message': 'P√©riode de fin d\'ann√©e: Anticiper la baisse des dons pendant les f√™tes.',
                'action': 'Constituer des r√©serves avant les f√™tes'
            },
            (3, 4, 5): {  # Printemps
                'type': 'SEASONAL_SPRING',
                'priority': 'LOW',
                'message': 'P√©riode favorable aux dons. Maintenir les campagnes r√©guli√®res.',
                'action': 'Continuer les op√©rations standard'
            }
        }

        for months, rec in seasonal_patterns.items():
            if month in months:
                rec['metric'] = month
                return rec

        return None

    def generate_summary_optimized(self, recommendations):
        """R√©sum√© optimis√© des recommandations"""
        if not recommendations:
            return {
                'total_recommendations': 0,
                'critical_count': 0,
                'high_priority_count': 0,
                'medium_priority_count': 0,
                'critical_blood_types': [],
                'overall_status': 'UNKNOWN'
            }

        # ==================== COMPTAGE PAR PRIORIT√â ====================
        priority_counts = {'CRITICAL': 0, 'HIGH': 0, 'MEDIUM': 0, 'LOW': 0, 'UNKNOWN': 0}
        critical_blood_types = []

        for rec in recommendations:
            priority = rec.get('priority', 'UNKNOWN')
            priority_counts[priority] += 1

            if priority in ['CRITICAL', 'HIGH']:
                critical_blood_types.append(rec['blood_type'])

        # ==================== STATUT GLOBAL ====================
        if priority_counts['CRITICAL'] > 0:
            overall_status = 'CRITICAL'
        elif priority_counts['HIGH'] > 2:
            overall_status = 'HIGH_ATTENTION'
        elif priority_counts['HIGH'] > 0 or priority_counts['MEDIUM'] > 3:
            overall_status = 'ATTENTION'
        else:
            overall_status = 'STABLE'

        return {
            'total_recommendations': len(recommendations),
            'critical_count': priority_counts['CRITICAL'],
            'high_priority_count': priority_counts['HIGH'],
            'medium_priority_count': priority_counts['MEDIUM'],
            'low_priority_count': priority_counts['LOW'],
            'critical_blood_types': critical_blood_types,
            'overall_status': overall_status,
            'summary_generated_at': timezone.now().isoformat()
        }

    def get_static_recommendations(self):
        """Recommandations statiques en cas d'√©chec total"""
        blood_types = ['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-']

        static_recs = []
        for bt in blood_types:
            static_recs.append(self.get_basic_recommendation(bt))

        return {
            'blood_type_specific': static_recs,
            'general': [{
                'type': 'SYSTEM_MAINTENANCE',
                'priority': 'LOW',
                'message': 'Syst√®me de recommandations en mode maintenance. Utiliser les proc√©dures manuelles.',
                'metric': 0,
                'action': 'V√©rifier manuellement les stocks critiques'
            }],
            'summary': {
                'total_recommendations': len(static_recs),
                'critical_count': 0,
                'high_priority_count': 0,
                'medium_priority_count': 0,
                'critical_blood_types': [],
                'overall_status': 'MAINTENANCE_MODE'
            }
        }

    def get_emergency_fallback(self):
        """Fallback d'urgence en cas d'erreur critique"""
        return Response({
            'recommendations': self.get_static_recommendations(),
            'generated_at': timezone.now().isoformat(),
            'status': 'emergency_fallback',
            'message': 'Syst√®me de recommandations en mode de secours. Fonctionnalit√©s limit√©es.',
            'cache_duration': '5 minutes'
        }, status=status.HTTP_206_PARTIAL_CONTENT)

@global_allow_any
# ==================== DATA IMPORT VIEWS ====================
@method_decorator(csrf_exempt, name='dispatch')
class DataImportAPIView(BaseAPIView):
    """Import des donn√©es CSV fournies par les organisateurs"""

    def post(self, request):
        try:
            csv_file = request.FILES.get('csv_file')
            if not csv_file:
                return Response(
                    {'error': 'Fichier CSV requis'},
                    status=status.HTTP_400_BAD_REQUEST
                )

            # Lire le fichier CSV
            file_data = csv_file.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(file_data))

            imported_count = 0
            errors = []

            with transaction.atomic():
                for row_num, row in enumerate(csv_reader, start=2):
                    try:
                        self.process_csv_row(row, row_num)
                        imported_count += 1
                    except Exception as e:
                        errors.append(f"Ligne {row_num}: {str(e)}")
                        if len(errors) > 50:  # Limiter les erreurs affich√©es
                            errors.append("... (plus d'erreurs tronqu√©es)")
                            break

            return Response({
                'success': True,
                'imported_records': imported_count,
                'errors': errors,
                'total_errors': len(errors)
            })

        except Exception as e:
            logger.error(f"CSV Import error: {str(e)}")
            return Response(
                {'error': f'Erreur lors de l\'import: {str(e)}'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

    def process_csv_row(self, row, row_num):
        """Traite une ligne du CSV"""
        # Nettoyer les donn√©es
        record_id = row.get('record_id', '').strip()
        donor_id = row.get('donor_id', '').strip()

        # Valeurs par d√©faut si manquantes
        if not record_id:
            record_id = f"BB{str(row_num).zfill(6)}"

        if not donor_id:
            donor_id = f"D{str(row_num).zfill(6)}"

        # Cr√©er ou r√©cup√©rer le site
        collection_site = row.get('collection_site', 'Site Inconnu').strip()
        if not collection_site:
            collection_site = 'Site Inconnu'

        site, _ = Site.objects.get_or_create(
            site_id=f"SITE_{collection_site.replace(' ', '_').upper()}",
            defaults={
                'nom': collection_site,
                'ville': 'Douala'  # Valeur par d√©faut
            }
        )

        # Cr√©er ou r√©cup√©rer le donneur
        donor_age = self.safe_float(row.get('donor_age'))
        donor_gender = row.get('donor_gender', 'M').strip()
        blood_type = row.get('blood_type', 'O+').strip()

        # Calculer date de naissance approximative
        birth_date = date.today() - timedelta(days=int(donor_age * 365)) if donor_age else date(1980, 1, 1)

        donor, _ = Donor.objects.get_or_create(
            donor_id=donor_id,
            defaults={
                'first_name': f'Donneur_{donor_id}',
                'last_name': 'Anonyme',
                'date_of_birth': birth_date,
                'gender': donor_gender if donor_gender in ['M', 'F'] else 'M',
                'blood_type': blood_type if blood_type in [choice[0] for choice in Donor.BLOOD_TYPE_CHOICES] else 'O+',
                'phone_number': '000000000'
            }
        )

        # Cr√©er l'enregistrement de don
        donation_date = self.parse_date(row.get('donation_date'))

        blood_record, _ = BloodRecord.objects.get_or_create(
            record_id=record_id,
            defaults={
                'site': site,
                'screening_results': 'Valid',
                'record_date': donation_date or date.today(),
                'quantity': 1
            }
        )

        # Cr√©er l'unit√© de sang
        unit_id = record_id  # Utiliser le m√™me ID
        collection_date = donation_date or date.today()
        expiry_date = self.parse_date(row.get('expiry_date'))

        # Si pas de date d'expiration, calculer (120 jours apr√®s collection)
        if not expiry_date:
            expiry_date = collection_date + timedelta(days=120)

        volume_ml = self.safe_int(row.get('collection_volume_ml'), 450)
        hemoglobin = self.safe_float(row.get('hemoglobin_g_dl'))

        # D√©terminer le statut
        status = 'Available'
        if expiry_date < date.today():
            status = 'Expired'

        BloodUnit.objects.get_or_create(
            unit_id=unit_id,
            defaults={
                'donor': donor,
                'record': blood_record,
                'collection_date': collection_date,
                'volume_ml': volume_ml,
                'hemoglobin_g_dl': hemoglobin,
                'date_expiration': expiry_date,
                'status': status
            }
        )

    def safe_int(self, value, default=0):
        """Conversion s√©curis√©e en entier"""
        try:
            return int(float(value)) if value and str(value).strip() else default
        except (ValueError, TypeError):
            return default

    def safe_float(self, value, default=None):
        """Conversion s√©curis√©e en float"""
        try:
            return float(value) if value and str(value).strip() else default
        except (ValueError, TypeError):
            return default

    def parse_date(self, date_string):
        """Parse une date au format YYYY-MM-DD"""
        if not date_string or not str(date_string).strip():
            return None

        try:
            return datetime.strptime(str(date_string).strip(), '%Y-%m-%d').date()
        except ValueError:
            try:
                return datetime.strptime(str(date_string).strip(), '%d/%m/%Y').date()
            except ValueError:
                return None

@global_allow_any
# ==================== CRUD VIEWS ====================
class BloodUnitListAPIView(generics.ListAPIView):
    """Liste des unit√©s de sang avec filtrage"""
    serializer_class = BloodUnitSerializer
    pagination_class = StandardResultsSetPagination

    def get_queryset(self):
        queryset = BloodUnit.objects.select_related('donor', 'record__site').all()

        # Filtres
        blood_type = self.request.query_params.get('blood_type')
        status = self.request.query_params.get('status')
        expiring_days = self.request.query_params.get('expiring_days')

        if blood_type:
            queryset = queryset.filter(donor__blood_type=blood_type)

        if status:
            queryset = queryset.filter(status=status)

        if expiring_days:
            try:
                days = int(expiring_days)
                expiry_threshold = timezone.now().date() + timedelta(days=days)
                queryset = queryset.filter(
                    status='Available',
                    date_expiration__lte=expiry_threshold
                )
            except ValueError:
                pass

        return queryset.order_by('-collection_date')

@global_allow_any
class BloodRequestListCreateAPIView(generics.ListCreateAPIView):
    """Liste et cr√©ation des demandes de sang"""
    serializer_class = BloodRequestSerializer
    pagination_class = StandardResultsSetPagination

    def get_queryset(self):
        queryset = BloodRequest.objects.select_related('department', 'site').all()

        # Filtres
        status = self.request.query_params.get('status')
        priority = self.request.query_params.get('priority')
        blood_type = self.request.query_params.get('blood_type')
        department = self.request.query_params.get('department')

        if status:
            queryset = queryset.filter(status=status)

        if priority:
            queryset = queryset.filter(priority=priority)

        if blood_type:
            queryset = queryset.filter(blood_type=blood_type)

        if department:
            queryset = queryset.filter(department__name__icontains=department)

        return queryset.order_by('-request_date', 'priority')






@global_allow_any
class BloodRequestDetailAPIView(generics.RetrieveUpdateDestroyAPIView):
    """D√©tail, mise √† jour et suppression d'une demande de sang"""
    queryset = BloodRequest.objects.select_related('department', 'site').all()
    serializer_class = BloodRequestSerializer
    lookup_field = 'request_id'

    def get_object(self):
        """R√©cup√©rer l'objet avec gestion d'erreur"""
        try:
            return super().get_object()
        except BloodRequest.DoesNotExist:
            from django.http import Http404
            raise Http404("Demande de sang non trouv√©e")


@global_allow_any
class BloodConsumptionListCreateAPIView(generics.ListCreateAPIView):
    """Liste et cr√©ation des consommations de sang"""
    serializer_class = BloodConsumptionSerializer
    pagination_class = StandardResultsSetPagination

    def get_queryset(self):
        queryset = BloodConsumption.objects.select_related(
            'request', 'unit__donor', 'patient'
        ).all()

        # Filtres
        date_from = self.request.query_params.get('date_from')
        date_to = self.request.query_params.get('date_to')
        blood_type = self.request.query_params.get('blood_type')

        if date_from:
            try:
                queryset = queryset.filter(date__gte=date_from)
            except ValueError:
                pass

        if date_to:
            try:
                queryset = queryset.filter(date__lte=date_to)
            except ValueError:
                pass

        if blood_type:
            queryset = queryset.filter(unit__donor__blood_type=blood_type)

        return queryset.order_by('-date')

@global_allow_any
# ==================== ANALYTICS VIEWS ====================
class InventoryAnalyticsAPIView(BaseAPIView):
    """Analytics avanc√©s des stocks avec cache Redis"""

    def get(self, request):
        period = request.GET.get('period', '30')  # jours

        try:
            days = int(period)

            # Cache sp√©cifique √† la p√©riode
            cache_key = cache_key_builder('inventory_analytics', period)
            cached_result = safe_cache_get(cache_key)

            if cached_result:
                logger.info(f"Analytics for {period} days served from Redis cache")
                return Response(cached_result)

            start_time = time.time()
            start_date = timezone.now().date() - timedelta(days=days)

            # √âvolution des stocks par groupe sanguin
            stock_evolution = self.get_stock_evolution_cached(start_date, days)

            # Taux d'utilisation
            utilization_rates = self.get_utilization_rates_cached(start_date)

            # Analyse des pertes - Version PostgreSQL
            waste_analysis = self.get_waste_analysis_cached(start_date)

            # Tendances de demande - Version PostgreSQL
            demand_trends = self.get_demand_trends_cached(start_date)

            # M√©triques de performance
            performance_metrics = self.get_performance_metrics_cached(start_date)

            execution_time = time.time() - start_time

            result = {
                'period_days': days,
                'stock_evolution': stock_evolution,
                'utilization_rates': utilization_rates,
                'waste_analysis': waste_analysis,
                'demand_trends': demand_trends,
                'performance_metrics': performance_metrics,
                'generated_at': timezone.now().isoformat(),
                'execution_time_seconds': round(execution_time, 2),
                'cache_backend': 'Redis Cloud'
            }

            # Cache pour 1 heure
            safe_cache_set(cache_key, result, timeout=3600)
            logger.info(f"Analytics generated and cached in {execution_time:.2f}s")

            return Response(result)

        except Exception as e:
            logger.error(f"Analytics error: {str(e)}")
            return Response(
                {'error': 'Erreur lors de la g√©n√©ration des analytics'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

    def get_stock_evolution_cached(self, start_date, days):
        """√âvolution des stocks avec cache par tranches"""
        cache_key = cache_key_builder('stock_evolution', start_date.isoformat(), days)
        cached_result = safe_cache_get(cache_key)

        if cached_result:
            return cached_result

        evolution = self.get_stock_evolution(start_date, days)
        safe_cache_set(cache_key, evolution, timeout=1800)  # 30 minutes
        return evolution

    def get_utilization_rates_cached(self, start_date):
        """Taux d'utilisation avec cache"""
        cache_key = cache_key_builder('utilization_rates', start_date.isoformat())
        cached_result = safe_cache_get(cache_key)

        if cached_result:
            return cached_result

        rates = self.get_utilization_rates(start_date)
        safe_cache_set(cache_key, rates, timeout=1800)  # 30 minutes
        return rates

    def get_waste_analysis_cached(self, start_date):
        """Analyse des pertes avec cache"""
        cache_key = cache_key_builder('waste_analysis', start_date.isoformat())
        cached_result = safe_cache_get(cache_key)

        if cached_result:
            return cached_result

        analysis = self.get_waste_analysis_postgresql(start_date)
        safe_cache_set(cache_key, analysis, timeout=1800)  # 30 minutes
        return analysis

    def get_demand_trends_cached(self, start_date):
        """Tendances de demande avec cache"""
        cache_key = cache_key_builder('demand_trends', start_date.isoformat())
        cached_result = safe_cache_get(cache_key)

        if cached_result:
            return cached_result

        trends = self.get_demand_trends_postgresql(start_date)
        safe_cache_set(cache_key, trends, timeout=1800)  # 30 minutes
        return trends

    def get_performance_metrics_cached(self, start_date):
        """M√©triques de performance avec cache"""
        cache_key = cache_key_builder('performance_metrics', start_date.isoformat())
        cached_result = safe_cache_get(cache_key)

        if cached_result:
            return cached_result

        metrics = self.get_performance_metrics(start_date)
        safe_cache_set(cache_key, metrics, timeout=1800)  # 30 minutes
        return metrics

    def get_stock_evolution(self, start_date, days):
        """√âvolution des stocks sur la p√©riode"""
        evolution = []

        for i in range(days):
            check_date = start_date + timedelta(days=i)

            daily_stocks = {}
            for blood_type in ['A+', 'A-', 'B+', 'B-', 'AB+', 'AB-', 'O+', 'O-']:
                stock_count = BloodUnit.objects.filter(
                    donor__blood_type=blood_type,
                    collection_date__lte=check_date,
                    date_expiration__gt=check_date
                ).count()
                daily_stocks[blood_type] = stock_count

            evolution.append({
                'date': check_date.isoformat(),
                'stocks': daily_stocks,
                'total': sum(daily_stocks.values())
            })

        return evolution

    def get_utilization_rates(self, start_date):
        """Taux d'utilisation par groupe sanguin"""
        rates = []

        for blood_type in ['A+', 'A-', 'B+', 'B-', 'AB+', 'AB-', 'O+', 'O-']:
            # Unit√©s collect√©es
            collected = BloodUnit.objects.filter(
                donor__blood_type=blood_type,
                collection_date__gte=start_date
            ).count()

            # Unit√©s utilis√©es
            used = BloodUnit.objects.filter(
                donor__blood_type=blood_type,
                collection_date__gte=start_date,
                status='Used'
            ).count()

            # Unit√©s expir√©es
            expired = BloodUnit.objects.filter(
                donor__blood_type=blood_type,
                collection_date__gte=start_date,
                status='Expired'
            ).count()

            utilization_rate = (used / collected * 100) if collected > 0 else 0
            waste_rate = (expired / collected * 100) if collected > 0 else 0

            rates.append({
                'blood_type': blood_type,
                'collected': collected,
                'used': used,
                'expired': expired,
                'utilization_rate': round(utilization_rate, 2),
                'waste_rate': round(waste_rate, 2)
            })

        return rates

    # Correction de la m√©thode get_waste_analysis_postgresql
    def get_waste_analysis_postgresql(self, start_date):
        """Analyse des pertes - Version corrig√©e"""
        try:
            # Utiliser TruncMonth correctement import√©
            expired_units = BloodUnit.objects.filter(
                status='Expired',
                date_expiration__gte=start_date
            ).annotate(
                month=TruncMonth('date_expiration')
            ).values('month', 'donor__blood_type').annotate(
                count=Count('unit_id'),
                total_volume=Sum('volume_ml')
            ).order_by('month')

            # Conversion en format lisible
            monthly_waste = []
            for item in expired_units:
                month_str = item['month'].strftime('%Y-%m') if item['month'] else 'Unknown'
                monthly_waste.append({
                    'month': month_str,
                    'blood_type': item['donor__blood_type'],
                    'count': item['count'],
                    'total_volume': item['total_volume'] or 0
                })

            # Co√ªt estim√© des pertes
            total_expired = sum(item['count'] for item in monthly_waste)
            estimated_cost = total_expired * 50000  # 50000 FCFA par unit√©

            return {
                'monthly_waste': monthly_waste,
                'total_expired_units': total_expired,
                'estimated_cost_fcfa': estimated_cost,
                'main_causes': [
                    {'cause': 'Expiration naturelle', 'percentage': 70},
                    {'cause': 'Probl√®mes de rotation', 'percentage': 20},
                    {'cause': 'D√©fauts de stockage', 'percentage': 10}
                ]
            }
        except Exception as e:
            logger.error(f"Waste analysis error: {str(e)}")
            return {
                'monthly_waste': [],
                'total_expired_units': 0,
                'estimated_cost_fcfa': 0,
                'main_causes': []
            }

    def get_demand_trends_postgresql(self, start_date):
        """Tendances de demande - Version corrig√©e"""
        try:
            # Utiliser TruncWeek correctement import√©
            weekly_demands = BloodRequest.objects.filter(
                request_date__gte=start_date
            ).annotate(
                week=TruncWeek('request_date'),
                year=Extract('request_date', 'year'),
                week_number=Extract('request_date', 'week')
            ).values('week', 'year', 'week_number', 'blood_type').annotate(
                total_quantity=Sum('quantity')
            ).order_by('week')

            # Conversion en format lisible
            weekly_trends = []
            for item in weekly_demands:
                week_str = f"{item['year']}-W{item['week_number']:02d}" if item['year'] and item[
                    'week_number'] else 'Unknown'
                weekly_trends.append({
                    'week': week_str,
                    'blood_type': item['blood_type'],
                    'total_quantity': item['total_quantity']
                })

            # Demandes par d√©partement
            dept_demands = BloodRequest.objects.filter(
                request_date__gte=start_date
            ).values('department__name').annotate(
                total_requests=Count('request_id'),
                total_quantity=Sum('quantity')
            ).order_by('-total_quantity')

            return {
                'weekly_trends': weekly_trends,
                'department_distribution': list(dept_demands),
                'peak_demand_days': self.get_peak_demand_days(start_date)
            }
        except Exception as e:
            logger.error(f"Demand trends error: {str(e)}")
            return {
                'weekly_trends': [],
                'department_distribution': [],
                'peak_demand_days': []
            }

    def get_peak_demand_days(self, start_date):
        """Jours de pic de demande"""
        try:
            daily_demands = BloodRequest.objects.filter(
                request_date__gte=start_date
            ).values('request_date').annotate(
                total_quantity=Sum('quantity')
            ).order_by('-total_quantity')[:10]

            return [
                {
                    'request_date': item['request_date'].isoformat() if item['request_date'] else None,
                    'total_quantity': item['total_quantity']
                }
                for item in daily_demands
            ]
        except Exception as e:
            logger.error(f"Peak demand days error: {str(e)}")
            return []

    def get_performance_metrics(self, start_date):
        """M√©triques de performance globales"""
        try:
            # Temps moyen de satisfaction des demandes
            fulfilled_requests = BloodRequest.objects.filter(
                request_date__gte=start_date,
                status='Fulfilled'
            )

            # Stock de s√©curit√© par groupe sanguin
            safety_stock_status = []
            for blood_type in ['A+', 'A-', 'B+', 'B-', 'AB+', 'AB-', 'O+', 'O-']:
                current_stock = BloodUnit.objects.filter(
                    donor__blood_type=blood_type,
                    status='Available'
                ).count()

                # Consommation moyenne des 7 derniers jours
                week_consumption = BloodConsumption.objects.filter(
                    unit__donor__blood_type=blood_type,
                    date__gte=timezone.now().date() - timedelta(days=7)
                ).count()

                avg_daily_consumption = week_consumption / 7
                days_of_supply = current_stock / avg_daily_consumption if avg_daily_consumption > 0 else float('inf')

                safety_stock_status.append({
                    'blood_type': blood_type,
                    'current_stock': current_stock,
                    'days_of_supply': round(days_of_supply, 1) if days_of_supply != float('inf') else 999,
                    'status': 'safe' if days_of_supply >= 7 else 'critical' if days_of_supply < 3 else 'warning'
                })

            total_requests = BloodRequest.objects.filter(request_date__gte=start_date).count()
            fulfillment_rate = round(fulfilled_requests.count() / total_requests * 100, 2) if total_requests > 0 else 0

            return {
                'total_requests': total_requests,
                'fulfilled_requests': fulfilled_requests.count(),
                'fulfillment_rate': fulfillment_rate,
                'safety_stock_status': safety_stock_status,
                'average_stock_turnover': self.calculate_stock_turnover(start_date)
            }
        except Exception as e:
            logger.error(f"Performance metrics error: {str(e)}")
            return {
                'total_requests': 0,
                'fulfilled_requests': 0,
                'fulfillment_rate': 0,
                'safety_stock_status': [],
                'average_stock_turnover': 0
            }

    def calculate_stock_turnover(self, start_date):
        """Calcule la rotation moyenne des stocks"""
        try:
            total_used = BloodUnit.objects.filter(
                collection_date__gte=start_date,
                status='Used'
            ).count()

            avg_stock = BloodUnit.objects.filter(
                collection_date__gte=start_date
            ).count() / 2  # Approximation du stock moyen

            return round(total_used / avg_stock, 2) if avg_stock > 0 else 0
        except:
            return 0


@global_allow_any
# ==================== REPORTING VIEWS ====================
class ReportExportAPIView(BaseAPIView):
    """Export de rapports en CSV"""

    def get(self, request):
        report_type = request.GET.get('type', 'inventory')
        format_type = request.GET.get('format', 'csv')

        try:
            if report_type == 'inventory':
                return self.export_inventory_report(format_type)
            elif report_type == 'consumption':
                return self.export_consumption_report(format_type)
            elif report_type == 'waste':
                return self.export_waste_report(format_type)
            elif report_type == 'donors':
                return self.export_donors_report(format_type)
            else:
                return Response(
                    {'error': 'Type de rapport non support√©'},
                    status=status.HTTP_400_BAD_REQUEST
                )

        except Exception as e:
            logger.error(f"Report export error: {str(e)}")
            return Response(
                {'error': 'Erreur lors de l\'export du rapport'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

    def export_inventory_report(self, format_type):
        """Export du rapport d'inventaire"""
        response = HttpResponse(content_type='text/csv')
        response[
            'Content-Disposition'] = f'attachment; filename="inventory_report_{timezone.now().strftime("%Y%m%d")}.csv"'

        writer = csv.writer(response)
        writer.writerow([
            'Unit ID', 'Blood Type', 'Status', 'Collection Date',
            'Expiry Date', 'Volume (ml)', 'Hemoglobin (g/dl)',
            'Site', 'Days to Expiry'
        ])

        units = BloodUnit.objects.select_related('donor', 'record__site').all()

        for unit in units:
            writer.writerow([
                unit.unit_id,
                unit.donor.blood_type,
                unit.status,
                unit.collection_date,
                unit.date_expiration,
                unit.volume_ml,
                unit.hemoglobin_g_dl or '',
                unit.record.site.nom,
                unit.days_until_expiry
            ])

        return response

    def export_consumption_report(self, format_type):
        """Export du rapport de consommation"""
        response = HttpResponse(content_type='text/csv')
        response[
            'Content-Disposition'] = f'attachment; filename="consumption_report_{timezone.now().strftime("%Y%m%d")}.csv"'

        writer = csv.writer(response)
        writer.writerow([
            'Date', 'Unit ID', 'Blood Type', 'Patient',
            'Department', 'Volume', 'Request ID'
        ])

        consumptions = BloodConsumption.objects.select_related(
            'unit__donor', 'patient', 'request__department'
        ).all()

        for consumption in consumptions:
            writer.writerow([
                consumption.date,
                consumption.unit.unit_id,
                consumption.unit.donor.blood_type,
                f"{consumption.patient.first_name} {consumption.patient.last_name}",
                consumption.request.department.name,
                consumption.volume,
                consumption.request.request_id
            ])

        return response

    def export_waste_report(self, format_type):
        """Export du rapport de gaspillage"""
        response = HttpResponse(content_type='text/csv')
        response[
            'Content-Disposition'] = f'attachment; filename="waste_report_{timezone.now().strftime("%Y%m%d")}.csv"'

        writer = csv.writer(response)
        writer.writerow([
            'Unit ID', 'Blood Type', 'Collection Date',
            'Expiry Date', 'Volume (ml)', 'Site', 'Days Expired'
        ])

        expired_units = BloodUnit.objects.filter(status='Expired').select_related('donor', 'record__site')

        for unit in expired_units:
            days_expired = (timezone.now().date() - unit.date_expiration).days
            writer.writerow([
                unit.unit_id,
                unit.donor.blood_type,
                unit.collection_date,
                unit.date_expiration,
                unit.volume_ml,
                unit.record.site.nom,
                days_expired
            ])

        return response

    def export_donors_report(self, format_type):
        """Export du rapport des donneurs"""
        response = HttpResponse(content_type='text/csv')
        response[
            'Content-Disposition'] = f'attachment; filename="donors_report_{timezone.now().strftime("%Y%m%d")}.csv"'

        writer = csv.writer(response)
        writer.writerow([
            'Donor ID', 'Name', 'Age', 'Gender', 'Blood Type',
            'Phone', 'Total Donations', 'Last Donation'
        ])

        donors = Donor.objects.all()

        for donor in donors:
            total_donations = BloodUnit.objects.filter(donor=donor).count()
            last_donation = BloodUnit.objects.filter(donor=donor).order_by('-collection_date').first()
            last_donation_date = last_donation.collection_date if last_donation else ''

            writer.writerow([
                donor.donor_id,
                f"{donor.first_name} {donor.last_name}",
                donor.age,
                donor.get_gender_display(),
                donor.blood_type,
                donor.phone_number,
                total_donations,
                last_donation_date
            ])

        return response

@global_allow_any
# ==================== CONFIGURATION VIEWS ====================
class SystemConfigAPIView(BaseAPIView):
    """Configuration du syst√®me"""

    def get(self, request):
        """R√©cup√©rer la configuration actuelle"""
        return Response({
            'blood_types': [choice[0] for choice in Donor.BLOOD_TYPE_CHOICES],
            'unit_statuses': [choice[0] for choice in BloodUnit.STATUS_CHOICES],
            'request_priorities': [choice[0] for choice in BloodRequest.PRIORITY_CHOICES],
            'default_expiry_days': 120,
            'minimum_stock_levels': {
                'A+': 10, 'A-': 5, 'B+': 8, 'B-': 4,
                'AB+': 3, 'AB-': 2, 'O+': 15, 'O-': 8
            },
            'alert_thresholds': {
                'low_stock': 5,
                'expiring_soon_days': 7,
                'critical_stock': 2
            }
        })

# ==================== ERROR HANDLERS ====================
from django.http import HttpResponse

def custom_404_view(request, exception):
    """Vue personnalis√©e pour les erreurs 404"""
    return JsonResponse({
        'error': 'Ressource non trouv√©e',
        'status_code': 404
    }, status=404)

def custom_500_view(request):
    """Vue personnalis√©e pour les erreurs 500"""
    return JsonResponse({
        'error': 'Erreur interne du serveur',
        'status_code': 500
    }, status=500)

# ==================== UTILITY FUNCTIONS ====================
def calculate_compatibility_matrix():
    """Calcule la matrice de compatibilit√© des groupes sanguins"""
    compatibility = {
        'A+': ['A+', 'AB+'],
        'A-': ['A+', 'A-', 'AB+', 'AB-'],
        'B+': ['B+', 'AB+'],
        'B-': ['B+', 'B-', 'AB+', 'AB-'],
        'AB+': ['AB+'],
        'AB-': ['AB+', 'AB-'],
        'O+': ['A+', 'B+', 'AB+', 'O+'],
        'O-': ['A+', 'A-', 'B+', 'B-', 'AB+', 'AB-', 'O+', 'O-']
    }
    return compatibility

@api_view(['GET'])
@csrf_exempt
def blood_compatibility(request):
    """API pour r√©cup√©rer la matrice de compatibilit√©"""
    return Response({
        'compatibility_matrix': calculate_compatibility_matrix(),
        'description': 'Groupes sanguins compatibles pour chaque donneur'
    })

# ==================== HEALTH CHECK ====================



# Dans views.py, ajouter ces vues apr√®s les vues existantes
@global_allow_any
# ==================== DONORS CRUD VIEWS ====================
class DonorListCreateAPIView(generics.ListCreateAPIView):
    """Liste et cr√©ation des donneurs"""
    serializer_class = DonorSerializer
    pagination_class = StandardResultsSetPagination

    def get_queryset(self):
        queryset = Donor.objects.all()
        search = self.request.query_params.get('search')
        blood_type = self.request.query_params.get('blood_type')

        if search:
            queryset = queryset.filter(
                Q(first_name__icontains=search) |
                Q(last_name__icontains=search) |
                Q(donor_id__icontains=search) |
                Q(phone_number__icontains=search)
            )

        if blood_type:
            queryset = queryset.filter(blood_type=blood_type)

        return queryset.order_by('-donor_id')

@global_allow_any
class DonorDetailAPIView(generics.RetrieveUpdateDestroyAPIView):
    """D√©tail, mise √† jour et suppression d'un donneur"""
    queryset = Donor.objects.all()
    serializer_class = DonorSerializer
    lookup_field = 'donor_id'

@global_allow_any
# ==================== PATIENTS CRUD VIEWS ====================
class PatientListCreateAPIView(generics.ListCreateAPIView):
    """Liste et cr√©ation des patients"""
    serializer_class = PatientSerializer
    pagination_class = StandardResultsSetPagination

    def get_queryset(self):
        queryset = Patient.objects.all()
        search = self.request.query_params.get('search')
        blood_type = self.request.query_params.get('blood_type')

        if search:
            queryset = queryset.filter(
                Q(first_name__icontains=search) |
                Q(last_name__icontains=search) |
                Q(patient_id__icontains=search)
            )

        if blood_type:
            queryset = queryset.filter(blood_type=blood_type)

        return queryset.order_by('-patient_id')

@global_allow_any
class PatientDetailAPIView(generics.RetrieveUpdateDestroyAPIView):
    """D√©tail, mise √† jour et suppression d'un patient"""
    queryset = Patient.objects.all()
    serializer_class = PatientSerializer
    lookup_field = 'patient_id'

@global_allow_any
# ==================== SITES CRUD VIEWS ====================
class SiteListCreateAPIView(generics.ListCreateAPIView):
    """Liste et cr√©ation des sites"""
    serializer_class = SiteSerializer
    pagination_class = StandardResultsSetPagination

    def get_queryset(self):
        queryset = Site.objects.all()
        search = self.request.query_params.get('search')

        if search:
            queryset = queryset.filter(
                Q(nom__icontains=search) |
                Q(ville__icontains=search) |
                Q(site_id__icontains=search)
            )

        return queryset.order_by('nom')

@global_allow_any
class SiteDetailAPIView(generics.RetrieveUpdateDestroyAPIView):
    """D√©tail, mise √† jour et suppression d'un site"""
    queryset = Site.objects.all()
    serializer_class = SiteSerializer
    lookup_field = 'site_id'

# 1. SOLUTION RAPIDE : D√©placer les fonctions dans le bon fichier

# Dans votre fichier views.py (paste.txt), ajoutez ces fonctions √† la fin :

# Dans blood_demand_forecasting.py - Fonction API am√©lior√©e

def generate_forecast_api(blood_type, days_ahead=7, method='auto', force_retrain=False):
    """
    üéØ FONCTION API PRINCIPALE - VERSION ROBUSTE
    G√©n√®re des pr√©visions avec gestion d'erreurs compl√®te
    """
    start_time = time.time()

    try:
        logger.info(f"üöÄ Starting AI forecast: {blood_type}, {days_ahead} days, method: {method}")

        # Validation des param√®tres
        if not blood_type or blood_type not in ['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-']:
            return {
                'error': 'Invalid blood type',
                'message': f'Blood type "{blood_type}" is not supported',
                'supported_types': ['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-']
            }

        if days_ahead < 1 or days_ahead > 30:
            return {
                'error': 'Invalid forecast period',
                'message': f'Days ahead must be between 1 and 30, got {days_ahead}'
            }

        valid_methods = ['auto', 'random_forest', 'xgboost', 'arima', 'stl_arima']
        if method not in valid_methods:
            return {
                'error': 'Invalid method',
                'message': f'Method "{method}" is not supported',
                'supported_methods': valid_methods
            }

        # Initialiser le forecaster
        try:
            forecaster = RealDataBloodDemandForecaster(max_execution_time=90)
            logger.info("‚úÖ Forecaster initialized successfully")
        except Exception as init_error:
            logger.error(f"‚ùå Forecaster initialization failed: {init_error}")
            return {
                'error': 'Forecaster initialization failed',
                'message': str(init_error),
                'timestamp': datetime.now().isoformat()
            }

        # V√©rifier les donn√©es disponibles avant de continuer
        try:
            test_data = forecaster.get_historical_data_from_db(blood_type, days_back=14)
            if test_data is None or len(test_data) < 7:
                logger.warning(
                    f"‚ö†Ô∏è Insufficient data for {blood_type}: {len(test_data) if test_data is not None else 0} days")
                return generate_minimal_forecast_with_context(blood_type, days_ahead, forecaster)

            logger.info(f"‚úÖ Data check passed: {len(test_data)} days available")

        except Exception as data_error:
            logger.error(f"‚ùå Data check failed: {data_error}")
            return generate_minimal_forecast_with_context(blood_type, days_ahead, forecaster)

        # Nettoyer le cache si force_retrain
        if force_retrain:
            try:
                forecaster.clear_model_cache(blood_type)
                logger.info(f"üßπ Cache cleared for {blood_type}")
            except Exception as cache_error:
                logger.warning(f"‚ö†Ô∏è Cache clearing failed: {cache_error}")

        # G√©n√©rer la pr√©vision
        try:
            result = forecaster.predict_with_real_data(blood_type, days_ahead, method)

            if not result or 'error' in result:
                logger.error(f"‚ùå Forecasting failed for {blood_type}")
                return generate_minimal_forecast_with_context(blood_type, days_ahead, forecaster)

            # Ajouter des m√©tadonn√©es
            result.update({
                'api_version': '2.0',
                'processing_time_seconds': round(time.time() - start_time, 2),
                'system_status': 'operational',
                'force_retrain': force_retrain
            })

            logger.info(f"‚úÖ Forecast completed for {blood_type} in {time.time() - start_time:.2f}s")
            return result

        except TimeoutException:
            logger.error(f"‚è∞ Forecast timeout for {blood_type}")
            return {
                'error': 'Forecast timeout',
                'message': f'Forecast generation exceeded maximum time limit',
                'blood_type': blood_type,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as forecast_error:
            logger.error(f"‚ùå Forecast generation error: {forecast_error}")
            return generate_minimal_forecast_with_context(blood_type, days_ahead, forecaster)

    except Exception as critical_error:
        logger.error(f"‚ùå Critical API error: {critical_error}")
        return {
            'error': 'Critical system error',
            'message': str(critical_error),
            'blood_type': blood_type,
            'timestamp': datetime.now().isoformat(),
            'processing_time_seconds': round(time.time() - start_time, 2)
        }

def generate_minimal_forecast_with_context(blood_type, days_ahead, forecaster):
    """
    üö® G√©n√®re une pr√©vision minimale mais utilisable avec contexte
    """
    try:
        logger.info(f"üîÑ Generating minimal forecast for {blood_type}")

        # Essayer de r√©cup√©rer des donn√©es contextuelles
        contextual_data = forecaster.get_contextual_data(blood_type)

        # Utiliser les moyennes r√©centes si disponibles
        recent_avg = contextual_data.get('recent_daily_avg', 0)
        monthly_avg = contextual_data.get('monthly_daily_avg', 0)

        # D√©terminer une demande de base r√©aliste
        if recent_avg > 0:
            base_demand = max(1, int(recent_avg))
        elif monthly_avg > 0:
            base_demand = max(1, int(monthly_avg))
        else:
            # Valeurs par d√©faut bas√©es sur la criticit√© du type sanguin
            critical_types = ['O-', 'AB+', 'AB-']
            base_demand = 3 if blood_type in critical_types else 2

        # G√©n√©rer les pr√©dictions avec pattern hebdomadaire
        predictions = []
        for i in range(days_ahead):
            future_date = datetime.now() + timedelta(days=i + 1)
            day_of_week = future_date.weekday()

            # Pattern hebdomadaire simple (moins de demande le weekend)
            if day_of_week in [5, 6]:  # Weekend
                daily_demand = max(1, int(base_demand * 0.7))
            elif day_of_week == 0:  # Lundi (souvent plus √©lev√©)
                daily_demand = max(1, int(base_demand * 1.2))
            else:
                daily_demand = base_demand

            # Ajouter une petite variation
            variation = int(daily_demand * 0.1) if daily_demand > 2 else 0
            if variation > 0:
                import random
                daily_demand += random.randint(-variation, variation)

            daily_demand = max(1, daily_demand)

            predictions.append({
                'date': future_date.strftime('%Y-%m-%d'),
                'predicted_demand': daily_demand,
                'confidence': max(0.3, 0.5 - (i * 0.02))  # Confiance d√©croissante
            })

        # Calculer des intervalles de confiance simples
        confidence_intervals = {
            'lower': [max(0, int(p['predicted_demand'] * 0.7)) for p in predictions],
            'upper': [int(p['predicted_demand'] * 1.4) for p in predictions],
            'margin': base_demand * 0.3
        }

        return {
            'blood_type': blood_type,
            'predictions': predictions,
            'method_used': 'minimal_contextual_forecast',
            'confidence_intervals': confidence_intervals,
            'generated_at': datetime.now().isoformat(),
            'data_source': 'limited_context',
            'warning': 'Pr√©vision bas√©e sur des donn√©es limit√©es',
            'contextual_insights': {
                'current_stock': contextual_data.get('current_stock', 0),
                'recent_trend': recent_avg,
                'data_availability': 'limited',
                'base_demand_used': base_demand
            },
            'model_performance': {
                'mape': 30.0,  # Estimation conservative
                'training_samples': 0,
                'confidence_level': 'low'
            },
            'optimization_recommendations': [
                {
                    'type': 'data_collection',
                    'priority': 'high',
                    'message': f'Collectez plus de donn√©es historiques pour {blood_type}',
                    'action': 'improve_data_collection'
                },
                {
                    'type': 'monitoring',
                    'priority': 'medium',
                    'message': f'Surveillez attentivement les niveaux de stock pour {blood_type}',
                    'action': 'increase_monitoring'
                }
            ]
        }

    except Exception as e:
        logger.error(f"‚ùå Minimal forecast failed: {e}")

        # Fallback ultime
        predictions = []
        for i in range(days_ahead):
            future_date = datetime.now() + timedelta(days=i + 1)
            predictions.append({
                'date': future_date.strftime('%Y-%m-%d'),
                'predicted_demand': 2,  # Valeur minimale s√ªre
                'confidence': 0.3
            })

        return {
            'blood_type': blood_type,
            'predictions': predictions,
            'method_used': 'emergency_fallback',
            'generated_at': datetime.now().isoformat(),
            'warning': 'Pr√©vision d\'urgence - donn√©es insuffisantes',
            'error_context': str(e)
        }

def get_available_methods():
    """
    üìã Retourne les m√©thodes disponibles avec leur statut
    """
    methods = {
        'auto': {
            'name': 'Automatique',
            'description': 'S√©lection automatique de la meilleure m√©thode',
            'available': True,
            'recommended': True
        },
        'random_forest': {
            'name': 'Random Forest',
            'description': 'Algorithme d\'ensemble robuste',
            'available': True,
            'good_for': 'Donn√©es stables avec peu de bruit'
        },
        'xgboost': {
            'name': 'XGBoost',
            'description': 'Gradient boosting avanc√©',
            'available': XGBOOST_AVAILABLE,
            'good_for': 'Donn√©es complexes avec patterns non-lin√©aires'
        },
        'arima': {
            'name': 'ARIMA',
            'description': 'Mod√®le de s√©rie temporelle classique',
            'available': STATSMODELS_AVAILABLE,
            'good_for': 'Donn√©es avec tendances claires'
        },
        'stl_arima': {
            'name': 'STL + ARIMA',
            'description': 'D√©composition saisonni√®re + ARIMA',
            'available': STATSMODELS_AVAILABLE,
            'good_for': 'Donn√©es avec saisonnalit√© marqu√©e'
        }
    }

    return {
        'available_methods': methods,
        'system_capabilities': {
            'xgboost_available': XGBOOST_AVAILABLE,
            'statsmodels_available': STATSMODELS_AVAILABLE,
            'max_forecast_days': 30,
            'supported_blood_types': ['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-']
        }
    }

def generate_recommendations(forecast_result):
    """
    üí° G√âN√âRATION DE RECOMMANDATIONS bas√©es sur les vraies pr√©dictions
    """
    try:
        if not forecast_result.get('predictions'):
            return []

        recommendations = []
        predictions = forecast_result['predictions']
        blood_type = forecast_result['blood_type']

        # Analyser les pr√©dictions
        if predictions:
            demands = [p.get('predicted_demand', 0) for p in predictions]
            max_demand = max(demands) if demands else 0
            avg_demand = sum(demands) / len(demands) if demands else 0

            # Stock actuel
            current_stock = forecast_result.get('contextual_insights', {}).get('current_stock', 0)

            # Recommandations bas√©es sur les vraies pr√©dictions
            if max_demand > avg_demand * 1.5:
                recommendations.append({
                    'type': 'demand_spike',
                    'priority': 'high',
                    'message': f"Pic de demande pr√©vu: {max_demand} unit√©s. Pr√©voir un stock suppl√©mentaire.",
                    'action': 'increase_collection'
                })

            if current_stock > 0:
                total_predicted = sum(demands)
                if total_predicted > 0:
                    days_coverage = current_stock / (total_predicted / len(demands))
                    if days_coverage < 3:
                        recommendations.append({
                            'type': 'low_stock',
                            'priority': 'critical',
                            'message': f"Stock critique pour {blood_type}. Collecte urgente recommand√©e.",
                            'action': 'urgent_collection'
                        })

        return recommendations

    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©ration recommandations: {e}")
        return []


@api_view(['GET'])
def health_check(request):
    """
    üè• HEALTH CHECK CORRIG√â - Sans timeout
    """
    start_time = time.time()

    try:
        # ==================== DATABASE CHECK RAPIDE ====================
        db_status = "unknown"
        db_info = {}

        try:
            # Test DB ultra-rapide avec timeout
            db_start = time.time()
            with connection.cursor() as cursor:
                cursor.execute("SELECT 1")
                result = cursor.fetchone()

            db_response_time = round((time.time() - db_start) * 1000, 2)
            db_status = "connected"
            db_info = {
                'response_time_ms': db_response_time,
                'test_query': 'SUCCESS'
            }

        except Exception as db_error:
            db_status = "error"
            db_info = {'error': str(db_error)[:100]}

        # ==================== CACHE CHECK RAPIDE ====================
        cache_status = "unknown"
        try:
            cache_test_key = f'health_{int(time.time())}'
            cache.set(cache_test_key, 'test', 10)
            cache.get(cache_test_key)
            cache.delete(cache_test_key)
            cache_status = "connected"
        except Exception:
            cache_status = "error"

        # ==================== AI SYSTEM CHECK MINIMAL ====================
        ai_status = "not_checked"
        ai_modules = {}

        try:
            # Test modules de base sans import lourd
            import pandas as pd
            import numpy as np
            from sklearn.ensemble import RandomForestRegressor

            ai_modules = {
                'pandas': {'available': True, 'version': pd.__version__},
                'numpy': {'available': True, 'version': np.__version__},
                'sklearn': {'available': True}
            }
            ai_status = "basic_available"

        except ImportError as e:
            ai_modules = {'basic_ml': {'available': False, 'error': str(e)[:50]}}
            ai_status = "limited"

        # ==================== RESPONSE RAPIDE ====================
        total_response_time = round((time.time() - start_time) * 1000, 2)

        # D√©terminer le statut global
        overall_status = "healthy"
        if db_status == "error":
            overall_status = "degraded"
        elif cache_status == "error":
            overall_status = "partial"

        response_data = {
            'status': overall_status,
            'timestamp': timezone.now().isoformat(),
            'version': '2.3-fixed',
            'response_time_ms': total_response_time,
            'components': {
                'database': {
                    'status': db_status,
                    'details': db_info
                },
                'cache': {
                    'status': cache_status
                },
                'ai_system': {
                    'status': ai_status,
                    'modules': ai_modules
                }
            },
            'system_info': {
                'python_version': sys.version.split()[0],
                'debug_mode': getattr(settings, 'DEBUG', False)
            }
        }

        # Status code selon l'√©tat
        if overall_status == "healthy":
            status_code = status.HTTP_200_OK
        elif overall_status == "partial":
            status_code = status.HTTP_206_PARTIAL_CONTENT
        else:
            status_code = status.HTTP_503_SERVICE_UNAVAILABLE

        return Response(response_data, status=status_code)

    except Exception as e:
        logger.error(f"Health check critical error: {e}")

        return Response({
            'status': 'critical_error',
            'error': str(e)[:200],
            'timestamp': timezone.now().isoformat(),
            'response_time_ms': round((time.time() - start_time) * 1000, 2),
            'version': '2.3-error-fallback'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


def custom_404_view(request, exception):
    """Custom 404 handler"""
    return JsonResponse({
        'error': 'Not Found',
        'message': 'The requested endpoint does not exist',
        'status_code': 404,
        'timestamp': timezone.now().isoformat()
    }, status=404)


def custom_500_view(request):
    """Custom 500 handler"""
    return JsonResponse({
        'error': 'Internal Server Error',
        'message': 'An unexpected error occurred',
        'status_code': 500,
        'timestamp': timezone.now().isoformat()
    }, status=500)


@api_view(['GET'])
def simple_health(request):
    """Health check ultra-simple en cas de probl√®me"""
    return JsonResponse({
        'status': 'alive',
        'timestamp': timezone.now().isoformat(),
        'method': request.method,
        'path': request.path
    })


# ==================== AI SYSTEM HEALTH (S√âPAR√â) ====================
@api_view(['GET'])
def ai_system_health(request):
    """Check du syst√®me AI s√©par√© pour √©viter les timeouts"""
    try:
        # Check si le module forecasting existe
        try:
            from forecasting.blood_demand_forecasting import health_check as ai_health
            result = ai_health()
            return Response(result, status=status.HTTP_200_OK)
        except ImportError:
            return Response({
                'status': 'not_available',
                'message': 'AI forecasting module not installed',
                'timestamp': timezone.now().isoformat(),
                'basic_ml_available': True  # sklearn est disponible
            }, status=status.HTTP_200_OK)
        except Exception as e:
            return Response({
                'status': 'error',
                'error': str(e),
                'timestamp': timezone.now().isoformat()
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    except Exception as e:
        return Response({
            'status': 'critical_error',
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# ==================== SYSTEM METRICS ====================
@api_view(['GET'])
def system_metrics(request):
    """M√©triques syst√®me basiques"""
    try:
        return Response({
            'status': 'operational',
            'timestamp': timezone.now().isoformat(),
            'metrics': {
                'database_status': 'connected',
                'cache_status': 'active',
                'api_status': 'operational'
            },
            'version': '2.3'
        }, status=status.HTTP_200_OK)

    except Exception as e:
        return Response({
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)